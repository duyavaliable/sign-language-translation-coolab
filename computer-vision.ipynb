{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1760171878235,
     "user": {
      "displayName": "Pham Hong Thai",
      "userId": "12210474276809954574"
     },
     "user_tz": -420
    },
    "id": "qRc-mv9RZepQ"
   },
   "outputs": [],
   "source": [
    "# cai dat cac thu vien can thiet\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "from google.colab import files\n",
    "from google.colab.patches import cv2_imshow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmRMpDyCbl9_"
   },
   "outputs": [],
   "source": [
    "# xu ly hinh anh\n",
    "def resize_image(image, target_size, preserve_aspect_ratio=True):\n",
    "    \"\"\"\n",
    "    Resize an image to the target size with aspect ratio preserved\n",
    "    \"\"\"\n",
    "    target_w, target_h = target_size\n",
    "\n",
    "    if isinstance(image, np.ndarray):\n",
    "        # OpenCV image\n",
    "        if preserve_aspect_ratio:\n",
    "            h0, w0 = image.shape[:2]\n",
    "            scale = min(target_w / w0, target_h / h0)\n",
    "            new_w, new_h = int(w0 * scale), int(h0 * scale)\n",
    "\n",
    "            # Resize theo tỉ lệ\n",
    "            resized = cv2.resize(image, (new_w, new_h))\n",
    "\n",
    "            # Tạo ảnh nền đen\n",
    "            if len(image.shape) == 3:\n",
    "                result = np.zeros((target_h, target_w, image.shape[2]), dtype=image.dtype)\n",
    "            else:\n",
    "                result = np.zeros((target_h, target_w), dtype=image.dtype)\n",
    "\n",
    "            # Tính toán padding để căn giữa\n",
    "            pad_w = target_w - new_w\n",
    "            pad_h = target_h - new_h\n",
    "            pad_left = pad_w // 2\n",
    "            pad_top = pad_h // 2\n",
    "\n",
    "            # Đặt ảnh resize vào giữa\n",
    "            if len(image.shape) == 3:\n",
    "                result[pad_top:pad_top+new_h, pad_left:pad_left+new_w, :] = resized\n",
    "            else:\n",
    "                result[pad_top:pad_top+new_h, pad_left:pad_left+new_w] = resized\n",
    "\n",
    "            return result\n",
    "        else:\n",
    "            return cv2.resize(image, target_size)\n",
    "    else:\n",
    "        # PIL Image\n",
    "        if preserve_aspect_ratio:\n",
    "            w0, h0 = image.size\n",
    "            scale = min(target_w / w0, target_h / h0)\n",
    "            new_w, new_h = int(w0 * scale), int(h0 * scale)\n",
    "\n",
    "            resized = image.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "            if image.mode == \"L\":\n",
    "                result = Image.new(\"L\", (target_w, target_h), 0)\n",
    "            else:\n",
    "                result = Image.new(\"RGB\", (target_w, target_h), (0, 0, 0))\n",
    "\n",
    "            x_offset = (target_w - new_w) // 2\n",
    "            y_offset = (target_h - new_h) // 2\n",
    "\n",
    "            result.paste(resized, (x_offset, y_offset))\n",
    "            return result\n",
    "        else:\n",
    "            return image.resize(target_size, Image.LANCZOS)\n",
    "\n",
    "def convert_to_grayscale(image):\n",
    "    \"\"\"\n",
    "    Convert an image to grayscale\n",
    "    \"\"\"\n",
    "    if isinstance(image, np.ndarray):\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        return image.convert(\"L\")\n",
    "\n",
    "def extract_hand_region(image, roi_size=400):\n",
    "    \"\"\"\n",
    "    Extract the hand region from the image using skin color detection\n",
    "    \"\"\"\n",
    "    # Define region of interest (ROI) in the center-right of the frame\n",
    "    height, width = image.shape[:2]\n",
    "    roi_x = width // 2 - roi_size // 2\n",
    "    roi_y = height // 2 - roi_size // 2\n",
    "\n",
    "    # Ensure ROI is within image bounds\n",
    "    roi_x = max(0, min(roi_x, width - roi_size))\n",
    "    roi_y = max(0, min(roi_y, height - roi_size))\n",
    "\n",
    "    # Extract ROI\n",
    "    roi = image[roi_y:roi_y+roi_size, roi_x:roi_x+roi_size].copy()\n",
    "\n",
    "    # Convert to HSV color space\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define range for skin color detection\n",
    "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "\n",
    "    # Create binary mask for skin color\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "\n",
    "    # Apply morphological operations to improve the mask\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "    mask = cv2.erode(mask, kernel, iterations=1)\n",
    "\n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Draw ROI boundary for visual guidance\n",
    "    cv2.rectangle(image, (roi_x, roi_y), (roi_x + roi_size, roi_y + roi_size),\n",
    "                 (255, 0, 0), 2)\n",
    "\n",
    "    if contours:\n",
    "        # Find the largest contour (assume it's the hand)\n",
    "        max_contour = max(contours, key=cv2.contourArea)\n",
    "        contour_area = cv2.contourArea(max_contour)\n",
    "\n",
    "        # If the contour is large enough (avoid small noise)\n",
    "        if contour_area > 3000:\n",
    "            # Get bounding rectangle\n",
    "            x, y, w, h = cv2.boundingRect(max_contour)\n",
    "\n",
    "            # Add some padding\n",
    "            padding = 20\n",
    "            x = max(0, x - padding)\n",
    "            y = max(0, y - padding)\n",
    "            w = min(roi_size - x, w + 2*padding)\n",
    "            h = min(roi_size - y, h + 2*padding)\n",
    "\n",
    "            # Extract the hand region from the ROI\n",
    "            hand = roi[y:y+h, x:x+w]\n",
    "\n",
    "            # Draw the rectangle on original image for visualization\n",
    "            cv2.rectangle(image, (roi_x + x, roi_y + y),\n",
    "                         (roi_x + x + w, roi_y + y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Display contour area for debugging\n",
    "            cv2.putText(image, f\"Area: {contour_area:.0f}\", (roi_x, roi_y - 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            if hand.size > 0:\n",
    "                # Thêm text khi phát hiện tay\n",
    "                cv2.putText(image, \"Hand Detected\", (roi_x, roi_y - 10),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                return hand\n",
    "\n",
    "    # If no good hand region is detected\n",
    "    cv2.putText(image, \"No hand detected\", (roi_x, roi_y - 10),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # Return a blurred version of ROI instead of the original one\n",
    "    blurred_roi = cv2.GaussianBlur(roi, (25, 25), 0)\n",
    "    return blurred_roi\n",
    "\n",
    "def draw_prediction(image, sign, confidence):\n",
    "    \"\"\"\n",
    "    Draw prediction text on the image\n",
    "    \"\"\"\n",
    "    # Create a copy of the image\n",
    "    result = image.copy()\n",
    "\n",
    "    # Draw a semi-transparent rectangle for text background\n",
    "    overlay = result.copy()\n",
    "    cv2.rectangle(overlay, (10, 10), (300, 140), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.6, result, 0.4, 0, result)\n",
    "\n",
    "    # Define text to display\n",
    "    if sign == \"?\" or confidence < 0.5:\n",
    "        text = \"Waiting for hand gesture...\"\n",
    "        color = (0, 0, 255)  # Red\n",
    "    else:\n",
    "        text = f\"Sign: {sign} ({confidence:.2f})\"\n",
    "        color = (0, 255, 0)  # Green\n",
    "\n",
    "    # Draw the text\n",
    "    cv2.putText(result, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    # Draw hand placement guide\n",
    "    height, width = image.shape[:2]\n",
    "    roi_size = 300\n",
    "    roi_x = width // 2 - roi_size // 2\n",
    "    roi_y = height // 2 - roi_size // 2\n",
    "    cv2.rectangle(result, (roi_x, roi_y), (roi_x + roi_size, roi_y + roi_size),\n",
    "                 (255, 0, 0), 2)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSKDTq2gb65e"
   },
   "outputs": [],
   "source": [
    "#mo hinh SignLanguageModel\n",
    "class SignLanguageModel:\n",
    "    def __init__(self, model_path=None):\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
    "                       'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "                       'space', 'delete', 'nothing']\n",
    "\n",
    "        if self.model_path:\n",
    "            self.load_model()\n",
    "        else:\n",
    "            self._create_model()\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"Create a CNN model for sign language recognition\"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            # Lớp Conv2D đầu tiên\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "            # Lớp Conv2D thứ hai\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "            # Lớp Conv2D thứ ba\n",
    "            tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "            # Flatten và các lớp fully connected\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(len(self.labels), activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        self.model = model\n",
    "        print(\"Created a new model. Note: This model is untrained.\")\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.model = tf.keras.models.load_model(self.model_path)\n",
    "            print(f\"Model loaded successfully from {self.model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Creating a new model instead...\")\n",
    "            self._create_model()\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Preprocess the input image for prediction\"\"\"\n",
    "        # Lấy hàm resize_image từ shared variables\n",
    "        resize_image = load_variable('resize_image')\n",
    "        if resize_image is None:\n",
    "            # Fallback nếu không tìm thấy hàm\n",
    "            from 2_Image_Processing import resize_image\n",
    "\n",
    "        # Convert OpenCV image to PIL Image\n",
    "        if len(image.shape) == 3:  # Color image\n",
    "            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        else:  # Already grayscale\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "        # Resize image\n",
    "        image = resize_image(image, (64, 64), preserve_aspect_ratio=True)\n",
    "\n",
    "        # Convert to grayscale if needed\n",
    "        if isinstance(image, Image.Image):\n",
    "            if image.mode != 'L':\n",
    "                image = image.convert('L')\n",
    "            img_array = np.array(image)\n",
    "        else:\n",
    "            if len(image.shape) == 3:\n",
    "                img_array = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            else:\n",
    "                img_array = image\n",
    "\n",
    "        # Normalize and reshape\n",
    "        img_array = img_array.astype('float32') / 255.0\n",
    "        img_array = img_array.reshape(1, 64, 64, 1)\n",
    "\n",
    "        return img_array\n",
    "\n",
    "    def predict_sign(self, image):\n",
    "        \"\"\"Predict the sign from an image\"\"\"\n",
    "        # Ensure image is grayscale\n",
    "        if len(image.shape) == 3:\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray_image = image\n",
    "\n",
    "        # Check if the image has enough variation to be a hand\n",
    "        std_dev = np.std(gray_image)\n",
    "        if std_dev < 20:  # If low variation, likely no hand\n",
    "            return \"?\", 0.0\n",
    "\n",
    "        # Process image\n",
    "        processed_image = self.preprocess_image(gray_image)\n",
    "\n",
    "        # Make prediction\n",
    "        predictions = self.model.predict(processed_image, verbose=0)\n",
    "\n",
    "        # Get the index of the highest confidence prediction\n",
    "        predicted_index = np.argmax(predictions[0])\n",
    "        confidence = predictions[0][predicted_index]\n",
    "\n",
    "        # Map the index to the sign\n",
    "        if predicted_index < 26:  # A-Z\n",
    "            predicted_sign = chr(65 + predicted_index)\n",
    "        else:\n",
    "            special_classes = {26: \"space\", 27: \"delete\", 28: \"nothing\"}\n",
    "            predicted_sign = special_classes.get(predicted_index, \"?\")\n",
    "\n",
    "        return predicted_sign, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J6GVs8kcVZN"
   },
   "outputs": [],
   "source": [
    "# huan luyen mo hinh\n",
    "def train_model(data_dir, epochs=None, batch_size=None):\n",
    "    \"\"\"\n",
    "    Train the sign language model with automatic parameter tuning\n",
    "    \"\"\"\n",
    "    print(f\"Training model with data from {data_dir}\")\n",
    "\n",
    "    # Ensure the data directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Error: Directory {data_dir} does not exist\")\n",
    "        return None\n",
    "\n",
    "    # Count samples and classes\n",
    "    total_images = 0\n",
    "    min_class_images = float('inf')\n",
    "    classes = []\n",
    "\n",
    "    for item in os.listdir(data_dir):\n",
    "        item_path = os.path.join(data_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            classes.append(item)\n",
    "            img_count = len([f for f in os.listdir(item_path)\n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            total_images += img_count\n",
    "            min_class_images = min(min_class_images, img_count)\n",
    "            print(f\"  - Class {item}: {img_count} images\")\n",
    "\n",
    "    print(f\"Total classes: {len(classes)}\")\n",
    "    print(f\"Total images: {total_images}\")\n",
    "    print(f\"Minimum images per class: {min_class_images}\")\n",
    "\n",
    "    if len(classes) == 0:\n",
    "        print(\"Error: No class directories found!\")\n",
    "        return None\n",
    "\n",
    "    # Auto-calculate batch_size and epochs if not provided\n",
    "    if batch_size is None:\n",
    "        if min_class_images <= 8:\n",
    "            batch_size = 1\n",
    "        elif min_class_images <= 16:\n",
    "            batch_size = 4\n",
    "        elif min_class_images <= 32:\n",
    "            batch_size = 8\n",
    "        elif min_class_images <= 64:\n",
    "            batch_size = 16\n",
    "        else:\n",
    "            batch_size = 32\n",
    "        print(f\"Auto-selected batch_size = {batch_size}\")\n",
    "\n",
    "    if epochs is None:\n",
    "        if total_images < 100:\n",
    "            epochs = 30\n",
    "        elif total_images < 500:\n",
    "            epochs = 20\n",
    "        elif total_images < 1000:\n",
    "            epochs = 15\n",
    "        else:\n",
    "            epochs = 10\n",
    "        print(f\"Auto-selected epochs = {epochs}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = SignLanguageModel()\n",
    "\n",
    "    # Load and prepare data\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=(64, 64),\n",
    "        batch_size=batch_size,\n",
    "        color_mode='grayscale',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=123,\n",
    "        image_size=(64, 64),\n",
    "        batch_size=batch_size,\n",
    "        color_mode='grayscale',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Normalize data\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "    train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    validation_ds = validation_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "    # Data augmentation - adjust strength based on dataset size\n",
    "    augmentation_strength = 0.2 if total_images > 1000 else (0.3 if total_images > 500 else 0.4)\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomRotation(augmentation_strength),\n",
    "        tf.keras.layers.RandomTranslation(augmentation_strength, augmentation_strength),\n",
    "        tf.keras.layers.RandomZoom(augmentation_strength),\n",
    "        tf.keras.layers.RandomContrast(augmentation_strength)\n",
    "    ])\n",
    "\n",
    "    # Apply augmentation\n",
    "    train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))\n",
    "\n",
    "    # Optimize performance\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    validation_ds = validation_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping_patience = 5 if total_images < 500 else 3\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=early_stopping_patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=0.2,\n",
    "            patience=2,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "     # Train the model\n",
    "    print(\"Starting model training...\")\n",
    "    model.model.fit(\n",
    "        train_ds,\n",
    "        validation_data=validation_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    model_file = \"models/sign_model.h5\"\n",
    "    model.model.save(model_file)\n",
    "    print(f\"Model saved to {model_file}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KFDlPhHcoZ9"
   },
   "outputs": [],
   "source": [
    "#danh gia mo hinh\n",
    "def visualize_dataset(data_dir='dataset', num_samples=3):\n",
    "    \"\"\"\n",
    "    Hiển thị một số mẫu từ dataset có sẵn\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Thư mục {data_dir} không tồn tại.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    classes = sorted([d for d in os.listdir(data_dir)\n",
    "                    if os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "\n",
    "    if not classes:\n",
    "        print(f\"Không tìm thấy lớp nào trong {data_dir}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Đếm tổng số lớp và tạo layout phù hợp\n",
    "    num_classes = len(classes)\n",
    "    fig = plt.figure(figsize=(12, 2*num_classes))\n",
    "\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        images = [f for f in os.listdir(class_dir)\n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "\n",
    "        if not images:\n",
    "            print(f\"Không tìm thấy ảnh nào trong lớp {class_name}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Lấy tất cả hoặc số lượng mẫu yêu cầu\n",
    "        samples = images[:num_samples] if len(images) > num_samples else images\n",
    "\n",
    "\n",
    "        for j, image_name in enumerate(samples):\n",
    "            # Tạo vị trí subplot\n",
    "            idx = i * num_samples + j + 1\n",
    "            ax = fig.add_subplot(num_classes, num_samples, idx)\n",
    "\n",
    "\n",
    "            # Đọc và hiển thị ảnh\n",
    "            image_path = os.path.join(class_dir, image_name)\n",
    "            img = cv2.imread(image_path)\n",
    "\n",
    "\n",
    "            if img is not None:\n",
    "                # Chuyển sang RGB để hiển thị đúng màu\n",
    "                if len(img.shape) == 3:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "                ax.imshow(img if len(img.shape) == 3 else img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                ax.set_title(f\"{class_name}\")\n",
    "                ax.axis('off')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/content/drive/MyDrive/Colab Notebooks/computer_vision/dataset'\n",
    "\n",
    "\n",
    "# # Để hiển thị một vài ảnh trong dataset:\n",
    "# visualize_dataset(dataset_path)\n",
    "\n",
    "\n",
    "# Để huấn luyện mô hình với dataset này:\n",
    "trained_model = train_model(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo mô hình và tải weights đã huấn luyện\n",
    "model_path = '/content/drive/MyDrive/Colab Notebooks/models/sign_model.h5'\n",
    "sign_model = SignLanguageModel(model_path=model_path)\n",
    "\n",
    "\n",
    "# Biên dịch lại mô hình sau khi tải\n",
    "# Sử dụng cùng optimizer, và metrics khi huấn luyện\n",
    "sign_model.model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print(\"Mô hình đã sẵn sàng để dự đoán.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải ảnh tĩnh lên Colab\n",
    "# uploaded = files.upload()\n",
    "\n",
    "\n",
    "# Lấy tên file ảnh đã tải lên\n",
    "img = cv2.imread(\"/content/drive/MyDrive/Colab Notebooks/computer_vision/chu A.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Đã tải lên ảnh: {img}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc ảnh từ file\n",
    "image = cv2.imread(\"/content/drive/MyDrive/Colab Notebooks/computer_vision/chu A.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if image is None:\n",
    "    print(f\"Lỗi: Không thể đọc ảnh từ đường dẫn {image}\")\n",
    "else:\n",
    "    # Trích xuất vùng tay từ ảnh\n",
    "    # extract_hand_region sẽ vẽ bounding box lên ảnh gốc\n",
    "    image_with_roi = image.copy() # Tạo bản sao để vẽ lên\n",
    "    hand_region = extract_hand_region(image_with_roi)\n",
    "\n",
    "\n",
    "    # Hiển thị ảnh với vùng ROI và bounding box (nếu tay được phát hiện)\n",
    "    print(\"Ảnh với vùng tay được phát hiện:\")\n",
    "    cv2_imshow(image_with_roi)\n",
    "\n",
    "\n",
    "    if hand_region is not None and hand_region.size > 0:\n",
    "        # Nếu trích xuất được vùng tay, tiến hành dự đoán\n",
    "        predicted_sign, confidence = sign_model.predict_sign(hand_region)\n",
    "\n",
    "\n",
    "        # Hiển thị kết quả dự đoán\n",
    "        print(f\"\\nKết quả dự đoán: {predicted_sign} (Độ tin cậy: {confidence:.2f})\")\n",
    "\n",
    "\n",
    "        # Vẽ kết quả dự đoán lên ảnh gốc\n",
    "        image_with_prediction = draw_prediction(image.copy(), predicted_sign, confidence)\n",
    "        print(\"\\nẢnh gốc với kết quả dự đoán:\")\n",
    "        cv2_imshow(image_with_prediction)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Nếu không trích xuất được vùng tay\n",
    "        print(\"\\nKhông phát hiện được vùng tay trong ảnh.\")\n",
    "        # Vẽ thông báo \"No hand detected\" lên ảnh gốc\n",
    "        image_with_no_hand = draw_prediction(image.copy(), \"?\", 0.0)\n",
    "        print(\"\\nẢnh gốc với thông báo:\")\n",
    "        cv2_imshow(image_with_no_hand)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPGxiA5tTOl1XFccjWeychP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
