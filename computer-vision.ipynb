{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGxiA5tTOl1XFccjWeychP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# cai dat cac thu vien can thiet\n","import os\n","import cv2\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","import io\n","import zipfile\n","\n","from google.colab import files\n","from google.colab.patches import cv2_imshow\n"],"metadata":{"id":"qRc-mv9RZepQ","executionInfo":{"status":"ok","timestamp":1760171878235,"user_tz":-420,"elapsed":3,"user":{"displayName":"Pham Hong Thai","userId":"12210474276809954574"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# xu ly hinh anh\n","def resize_image(image, target_size, preserve_aspect_ratio=True):\n","    \"\"\"\n","    Resize an image to the target size with aspect ratio preserved\n","    \"\"\"\n","    target_w, target_h = target_size\n","\n","    if isinstance(image, np.ndarray):\n","        # OpenCV image\n","        if preserve_aspect_ratio:\n","            h0, w0 = image.shape[:2]\n","            scale = min(target_w / w0, target_h / h0)\n","            new_w, new_h = int(w0 * scale), int(h0 * scale)\n","\n","            # Resize theo tỉ lệ\n","            resized = cv2.resize(image, (new_w, new_h))\n","\n","            # Tạo ảnh nền đen\n","            if len(image.shape) == 3:\n","                result = np.zeros((target_h, target_w, image.shape[2]), dtype=image.dtype)\n","            else:\n","                result = np.zeros((target_h, target_w), dtype=image.dtype)\n","\n","            # Tính toán padding để căn giữa\n","            pad_w = target_w - new_w\n","            pad_h = target_h - new_h\n","            pad_left = pad_w // 2\n","            pad_top = pad_h // 2\n","\n","            # Đặt ảnh resize vào giữa\n","            if len(image.shape) == 3:\n","                result[pad_top:pad_top+new_h, pad_left:pad_left+new_w, :] = resized\n","            else:\n","                result[pad_top:pad_top+new_h, pad_left:pad_left+new_w] = resized\n","\n","            return result\n","        else:\n","            return cv2.resize(image, target_size)\n","    else:\n","        # PIL Image\n","        if preserve_aspect_ratio:\n","            w0, h0 = image.size\n","            scale = min(target_w / w0, target_h / h0)\n","            new_w, new_h = int(w0 * scale), int(h0 * scale)\n","\n","            resized = image.resize((new_w, new_h), Image.LANCZOS)\n","\n","            if image.mode == \"L\":\n","                result = Image.new(\"L\", (target_w, target_h), 0)\n","            else:\n","                result = Image.new(\"RGB\", (target_w, target_h), (0, 0, 0))\n","\n","            x_offset = (target_w - new_w) // 2\n","            y_offset = (target_h - new_h) // 2\n","\n","            result.paste(resized, (x_offset, y_offset))\n","            return result\n","        else:\n","            return image.resize(target_size, Image.LANCZOS)\n","\n","def convert_to_grayscale(image):\n","    \"\"\"\n","    Convert an image to grayscale\n","    \"\"\"\n","    if isinstance(image, np.ndarray):\n","        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    else:\n","        return image.convert(\"L\")\n","\n","def extract_hand_region(image, roi_size=300):\n","    \"\"\"\n","    Extract the hand region from the image using skin color detection\n","    \"\"\"\n","    # Define region of interest (ROI) in the center-right of the frame\n","    height, width = image.shape[:2]\n","    roi_x = width // 2 - roi_size // 2\n","    roi_y = height // 2 - roi_size // 2\n","\n","    # Ensure ROI is within image bounds\n","    roi_x = max(0, min(roi_x, width - roi_size))\n","    roi_y = max(0, min(roi_y, height - roi_size))\n","\n","    # Extract ROI\n","    roi = image[roi_y:roi_y+roi_size, roi_x:roi_x+roi_size].copy()\n","\n","    # Convert to HSV color space\n","    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n","\n","    # Define range for skin color detection\n","    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n","    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n","\n","    # Create binary mask for skin color\n","    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n","\n","    # Apply morphological operations to improve the mask\n","    kernel = np.ones((5, 5), np.uint8)\n","    mask = cv2.dilate(mask, kernel, iterations=2)\n","    mask = cv2.erode(mask, kernel, iterations=1)\n","\n","    # Apply Gaussian blur to reduce noise\n","    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n","\n","    # Find contours in the mask\n","    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Draw ROI boundary for visual guidance\n","    cv2.rectangle(image, (roi_x, roi_y), (roi_x + roi_size, roi_y + roi_size),\n","                 (255, 0, 0), 2)\n","\n","    if contours:\n","        # Find the largest contour (assume it's the hand)\n","        max_contour = max(contours, key=cv2.contourArea)\n","        contour_area = cv2.contourArea(max_contour)\n","\n","        # If the contour is large enough (avoid small noise)\n","        if contour_area > 3000:\n","            # Get bounding rectangle\n","            x, y, w, h = cv2.boundingRect(max_contour)\n","\n","            # Add some padding\n","            padding = 20\n","            x = max(0, x - padding)\n","            y = max(0, y - padding)\n","            w = min(roi_size - x, w + 2*padding)\n","            h = min(roi_size - y, h + 2*padding)\n","\n","            # Extract the hand region from the ROI\n","            hand = roi[y:y+h, x:x+w]\n","\n","            # Draw the rectangle on original image for visualization\n","            cv2.rectangle(image, (roi_x + x, roi_y + y),\n","                         (roi_x + x + w, roi_y + y + h), (0, 255, 0), 2)\n","\n","            # Display contour area for debugging\n","            cv2.putText(image, f\"Area: {contour_area:.0f}\", (roi_x, roi_y - 30),\n","                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n","\n","            if hand.size > 0:\n","                # Thêm text khi phát hiện tay\n","                cv2.putText(image, \"Hand Detected\", (roi_x, roi_y - 10),\n","                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n","                return hand\n","\n","    # If no good hand region is detected\n","    cv2.putText(image, \"No hand detected\", (roi_x, roi_y - 10),\n","               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n","\n","    # Return a blurred version of ROI instead of the original one\n","    blurred_roi = cv2.GaussianBlur(roi, (25, 25), 0)\n","    return blurred_roi\n","\n","def draw_prediction(image, sign, confidence):\n","    \"\"\"\n","    Draw prediction text on the image\n","    \"\"\"\n","    # Create a copy of the image\n","    result = image.copy()\n","\n","    # Draw a semi-transparent rectangle for text background\n","    overlay = result.copy()\n","    cv2.rectangle(overlay, (10, 10), (300, 140), (0, 0, 0), -1)\n","    cv2.addWeighted(overlay, 0.6, result, 0.4, 0, result)\n","\n","    # Define text to display\n","    if sign == \"?\" or confidence < 0.5:\n","        text = \"Waiting for hand gesture...\"\n","        color = (0, 0, 255)  # Red\n","    else:\n","        text = f\"Sign: {sign} ({confidence:.2f})\"\n","        color = (0, 255, 0)  # Green\n","\n","    # Draw the text\n","    cv2.putText(result, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n","\n","    # Draw hand placement guide\n","    height, width = image.shape[:2]\n","    roi_size = 300\n","    roi_x = width // 2 - roi_size // 2\n","    roi_y = height // 2 - roi_size // 2\n","    cv2.rectangle(result, (roi_x, roi_y), (roi_x + roi_size, roi_y + roi_size),\n","                 (255, 0, 0), 2)\n","\n","    return result"],"metadata":{"id":"mmRMpDyCbl9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#mo hinh SignLanguageModel\n","class SignLanguageModel:\n","    def __init__(self, model_path=None):\n","        self.model_path = model_path\n","        self.model = None\n","        self.labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n","                       'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n","                       'space', 'delete', 'nothing']\n","\n","        if self.model_path:\n","            self.load_model()\n","        else:\n","            self._create_model()\n","\n","    def _create_model(self):\n","        \"\"\"Create a CNN model for sign language recognition\"\"\"\n","        model = tf.keras.Sequential([\n","            # Lớp Conv2D đầu tiên\n","            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n","            tf.keras.layers.MaxPooling2D((2, 2)),\n","\n","            # Lớp Conv2D thứ hai\n","            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","            tf.keras.layers.MaxPooling2D((2, 2)),\n","\n","            # Lớp Conv2D thứ ba\n","            tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n","            tf.keras.layers.MaxPooling2D((2, 2)),\n","\n","            # Flatten và các lớp fully connected\n","            tf.keras.layers.Flatten(),\n","            tf.keras.layers.Dense(128, activation='relu'),\n","            tf.keras.layers.Dropout(0.5),\n","            tf.keras.layers.Dense(len(self.labels), activation='softmax')\n","        ])\n","\n","        model.compile(optimizer='adam',\n","                      loss='sparse_categorical_crossentropy',\n","                      metrics=['accuracy'])\n","\n","        self.model = model\n","        print(\"Created a new model. Note: This model is untrained.\")\n","\n","    def load_model(self):\n","        try:\n","            self.model = tf.keras.models.load_model(self.model_path)\n","            print(f\"Model loaded successfully from {self.model_path}\")\n","        except Exception as e:\n","            print(f\"Error loading model: {e}\")\n","            print(\"Creating a new model instead...\")\n","            self._create_model()\n","\n","    def preprocess_image(self, image):\n","        \"\"\"Preprocess the input image for prediction\"\"\"\n","        # Lấy hàm resize_image từ shared variables\n","        resize_image = load_variable('resize_image')\n","        if resize_image is None:\n","            # Fallback nếu không tìm thấy hàm\n","            from 2_Image_Processing import resize_image\n","\n","        # Convert OpenCV image to PIL Image\n","        if len(image.shape) == 3:  # Color image\n","            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","        else:  # Already grayscale\n","            image = Image.fromarray(image)\n","\n","        # Resize image\n","        image = resize_image(image, (64, 64), preserve_aspect_ratio=True)\n","\n","        # Convert to grayscale if needed\n","        if isinstance(image, Image.Image):\n","            if image.mode != 'L':\n","                image = image.convert('L')\n","            img_array = np.array(image)\n","        else:\n","            if len(image.shape) == 3:\n","                img_array = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","            else:\n","                img_array = image\n","\n","        # Normalize and reshape\n","        img_array = img_array.astype('float32') / 255.0\n","        img_array = img_array.reshape(1, 64, 64, 1)\n","\n","        return img_array\n","\n","    def predict_sign(self, image):\n","        \"\"\"Predict the sign from an image\"\"\"\n","        # Ensure image is grayscale\n","        if len(image.shape) == 3:\n","            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","        else:\n","            gray_image = image\n","\n","        # Check if the image has enough variation to be a hand\n","        std_dev = np.std(gray_image)\n","        if std_dev < 20:  # If low variation, likely no hand\n","            return \"?\", 0.0\n","\n","        # Process image\n","        processed_image = self.preprocess_image(gray_image)\n","\n","        # Make prediction\n","        predictions = self.model.predict(processed_image, verbose=0)\n","\n","        # Get the index of the highest confidence prediction\n","        predicted_index = np.argmax(predictions[0])\n","        confidence = predictions[0][predicted_index]\n","\n","        # Map the index to the sign\n","        if predicted_index < 26:  # A-Z\n","            predicted_sign = chr(65 + predicted_index)\n","        else:\n","            special_classes = {26: \"space\", 27: \"delete\", 28: \"nothing\"}\n","            predicted_sign = special_classes.get(predicted_index, \"?\")\n","\n","        return predicted_sign, confidence"],"metadata":{"id":"wSKDTq2gb65e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# huan luyen mo hinh\n","def train_model(data_dir, epochs=None, batch_size=None):\n","    \"\"\"\n","    Train the sign language model with automatic parameter tuning\n","    \"\"\"\n","    print(f\"Training model with data from {data_dir}\")\n","\n","    # Ensure the data directory exists\n","    if not os.path.exists(data_dir):\n","        print(f\"Error: Directory {data_dir} does not exist\")\n","        return None\n","\n","    # Count samples and classes\n","    total_images = 0\n","    min_class_images = float('inf')\n","    classes = []\n","\n","    for item in os.listdir(data_dir):\n","        item_path = os.path.join(data_dir, item)\n","        if os.path.isdir(item_path):\n","            classes.append(item)\n","            img_count = len([f for f in os.listdir(item_path)\n","                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n","            total_images += img_count\n","            min_class_images = min(min_class_images, img_count)\n","            print(f\"  - Class {item}: {img_count} images\")\n","\n","    print(f\"Total classes: {len(classes)}\")\n","    print(f\"Total images: {total_images}\")\n","    print(f\"Minimum images per class: {min_class_images}\")\n","\n","    if len(classes) == 0:\n","        print(\"Error: No class directories found!\")\n","        return None\n","\n","    # Auto-calculate batch_size and epochs if not provided\n","    if batch_size is None:\n","        if min_class_images <= 8:\n","            batch_size = 1\n","        elif min_class_images <= 16:\n","            batch_size = 4\n","        elif min_class_images <= 32:\n","            batch_size = 8\n","        elif min_class_images <= 64:\n","            batch_size = 16\n","        else:\n","            batch_size = 32\n","        print(f\"Auto-selected batch_size = {batch_size}\")\n","\n","    if epochs is None:\n","        if total_images < 100:\n","            epochs = 30\n","        elif total_images < 500:\n","            epochs = 20\n","        elif total_images < 1000:\n","            epochs = 15\n","        else:\n","            epochs = 10\n","        print(f\"Auto-selected epochs = {epochs}\")\n","\n","    # Initialize model\n","    model = SignLanguageModel()\n","\n","    # Load and prepare data\n","    train_ds = tf.keras.utils.image_dataset_from_directory(\n","        data_dir,\n","        validation_split=0.2,\n","        subset=\"training\",\n","        seed=123,\n","        image_size=(64, 64),\n","        batch_size=batch_size,\n","        color_mode='grayscale',\n","        shuffle=True\n","    )\n","\n","    validation_ds = tf.keras.utils.image_dataset_from_directory(\n","        data_dir,\n","        validation_split=0.2,\n","        subset=\"validation\",\n","        seed=123,\n","        image_size=(64, 64),\n","        batch_size=batch_size,\n","        color_mode='grayscale',\n","        shuffle=True\n","    )\n","\n","    # Normalize data\n","    normalization_layer = tf.keras.layers.Rescaling(1./255)\n","    train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n","    validation_ds = validation_ds.map(lambda x, y: (normalization_layer(x), y))\n","\n","    # Data augmentation - adjust strength based on dataset size\n","    augmentation_strength = 0.2 if total_images > 1000 else (0.3 if total_images > 500 else 0.4)\n","    data_augmentation = tf.keras.Sequential([\n","        tf.keras.layers.RandomRotation(augmentation_strength),\n","        tf.keras.layers.RandomTranslation(augmentation_strength, augmentation_strength),\n","        tf.keras.layers.RandomZoom(augmentation_strength),\n","        tf.keras.layers.RandomContrast(augmentation_strength)\n","    ])\n","\n","    # Apply augmentation\n","    train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))\n","\n","    # Optimize performance\n","    AUTOTUNE = tf.data.AUTOTUNE\n","    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n","    validation_ds = validation_ds.prefetch(buffer_size=AUTOTUNE)\n","\n","    # Callbacks\n","    early_stopping_patience = 5 if total_images < 500 else 3\n","    callbacks = [\n","        tf.keras.callbacks.EarlyStopping(\n","            patience=early_stopping_patience,\n","            restore_best_weights=True,\n","            verbose=1\n","        ),\n","        tf.keras.callbacks.ReduceLROnPlateau(\n","            factor=0.2,\n","            patience=2,\n","            verbose=1\n","        )\n","    ]\n","\n","     # Train the model\n","    print(\"Starting model training...\")\n","    model.model.fit(\n","        train_ds,\n","        validation_data=validation_ds,\n","        epochs=epochs,\n","        callbacks=callbacks,\n","        verbose=1\n","    )\n","\n","    # Save the model\n","    model_file = \"models/sign_model.h5\"\n","    model.model.save(model_file)\n","    print(f\"Model saved to {model_file}\")\n","\n","    return model"],"metadata":{"id":"9J6GVs8kcVZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_KFDlPhHcoZ9"},"execution_count":null,"outputs":[]}]}