# -*- coding: utf-8 -*-
"""
Sign Language Translation - YOLOv8 Segmentation
Optimized for Local Environment
"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO

# ==================== C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ====================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATASET_DIR = os.path.join(BASE_DIR, 'dataset')
MODELS_DIR = os.path.join(BASE_DIR, 'models')
TRAIN_DIR = os.path.join(BASE_DIR, 'train')

# ==================== KH·ªûI T·∫†O MODEL ====================
print("üîÑ ƒêang kh·ªüi t·∫°o YOLO model...")
yolo_model = None  # S·∫Ω load khi c·∫ßn thi·∫øt
current_model_path = None  # Track model ƒëang d√πng

def get_yolo_model(model_path='yolov8n-seg.pt'):
    """
    Lazy loading YOLO model v·ªõi kh·∫£ nƒÉng reload
    
    Args:
        model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn YOLO model (m·∫∑c ƒë·ªãnh: yolov8n-seg.pt)
    
    Returns:
        model: YOLO model instance
    """
    global yolo_model, current_model_path
    
    # Reload n·∫øu model_path kh√°c
    if yolo_model is None or current_model_path != model_path:
        if not os.path.exists(model_path):
            print(f"‚ö†Ô∏è Warning: Model '{model_path}' kh√¥ng t·ªìn t·∫°i.")
            print(f"   Ultralytics s·∫Ω t·ª± ƒë·ªông t·∫£i pretrained model t·ª´ internet.")
        
        yolo_model = YOLO(model_path)
        current_model_path = model_path
        print(f"‚úì ƒê√£ t·∫£i YOLOv8 Segmentation model: {model_path}")
    
    return yolo_model

# ==================== CH·ª®C NƒÇNG X·ª¨ L√ù ·∫¢NH ====================
def extract_hand_region(image, model_path='yolov8n-seg.pt'):
    """
    Tr√≠ch xu·∫•t v√πng tay t·ª´ ·∫£nh s·ª≠ d·ª•ng YOLO segmentation
    
    Args:
        image: ·∫¢nh ƒë·∫ßu v√†o (numpy array)
        model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn YOLO segmentation model
    
    Returns:
        hand_box: V√πng ·∫£nh ch·ª©a b√†n tay ho·∫∑c ·∫£nh tr·ªëng n·∫øu kh√¥ng ph√°t hi·ªán
    """
    model = get_yolo_model(model_path)
    results = model(image, verbose=False)
    
    # L·∫•y boxes t·ª´ segmentation results
    if hasattr(results[0], 'boxes') and len(results[0].boxes) > 0:
        boxes = results[0].boxes.xyxy.cpu().numpy()
        
        # Ch·ªçn box l·ªõn nh·∫•t (gi·∫£ s·ª≠ l√† tay)
        areas = [(x2-x1)*(y2-y1) for x1, y1, x2, y2 in boxes]
        idx = areas.index(max(areas))
        x1, y1, x2, y2 = boxes[idx].astype(int)
        hand_box = image[y1:y2, x1:x2]
        
        # V·∫Ω bounding box
        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(image, "Hand Detected (YOLOv8-seg)", (x1, y1-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        return hand_box
    
    # Kh√¥ng ph√°t hi·ªán tay
    cv2.putText(image, "No hand detected", (20, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    return np.zeros((64, 64, 3), dtype=np.uint8)

def draw_prediction(image, sign, confidence):
    """
    V·∫Ω k·∫øt qu·∫£ d·ª± ƒëo√°n l√™n ·∫£nh
    
    Args:
        image: ·∫¢nh g·ªëc
        sign: K√Ω hi·ªáu ƒë∆∞·ª£c d·ª± ƒëo√°n
        confidence: ƒê·ªô tin c·∫≠y
    
    Returns:
        result: ·∫¢nh ƒë√£ v·∫Ω k·∫øt qu·∫£
    """
    result = image.copy()
    
    # V·∫Ω n·ªÅn cho text
    overlay = result.copy()
    cv2.rectangle(overlay, (10, 10), (300, 140), (0, 0, 0), -1)
    cv2.addWeighted(overlay, 0.6, result, 0.4, 0, result)
    
    # X√°c ƒë·ªãnh text v√† m√†u
    if sign == "?" or confidence < 0.5:
        text = "Waiting for hand gesture..."
        color = (0, 0, 255)  # Red
    else:
        text = f"Sign: {sign} ({confidence:.2f})"
        color = (0, 255, 0)  # Green
    
    cv2.putText(result, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
    
    # V·∫Ω khung h∆∞·ªõng d·∫´n ƒë·∫∑t tay
    height, width = image.shape[:2]
    roi_size = 300
    roi_x = width // 2 - roi_size // 2
    roi_y = height // 2 - roi_size // 2
    cv2.rectangle(result, (roi_x, roi_y), (roi_x + roi_size, roi_y + roi_size),
                 (255, 0, 0), 2)
    
    return result

# ==================== HI·ªÇN TH·ªä DATASET ====================
def visualize_dataset(data_dir=None, num_samples=3):
    """
    Hi·ªÉn th·ªã m·∫´u t·ª´ dataset
    H·ªó tr·ª£ c·∫£ c·∫•u tr√∫c flat (images tr·ª±c ti·∫øp) v√† per-class (subfolders)
    
    Args:
        data_dir: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c dataset
        num_samples: S·ªë l∆∞·ª£ng m·∫´u hi·ªÉn th·ªã cho m·ªói class
    """
    if data_dir is None:
        data_dir = os.path.join(DATASET_DIR, 'train', 'images')
    
    if not os.path.exists(data_dir):
        print(f"‚ùå Th∆∞ m·ª•c {data_dir} kh√¥ng t·ªìn t·∫°i.")
        return
    
    # Ki·ªÉm tra c·∫•u tr√∫c: flat ho·∫∑c per-class
    image_files = [f for f in os.listdir(data_dir) 
                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    if image_files:
        # C·∫•u tr√∫c flat: t·∫•t c·∫£ ·∫£nh trong images/
        print(f"üìÅ Dataset structure: FLAT (t·∫•t c·∫£ ·∫£nh trong {data_dir})")
        classes = ['all_images']
        class_images = {classes[0]: image_files}
    else:
        # C·∫•u tr√∫c per-class: images/<class>/*.jpg
        classes = sorted([d for d in os.listdir(data_dir)
                        if os.path.isdir(os.path.join(data_dir, d))])
        
        if not classes:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh ho·∫∑c class trong {data_dir}")
            return
        
        print(f"üìÅ Dataset structure: PER-CLASS ({len(classes)} classes)")
        class_images = {}
        for cls in classes:
            class_dir = os.path.join(data_dir, cls)
            imgs = [f for f in os.listdir(class_dir)
                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            class_images[cls] = imgs
    
    # Hi·ªÉn th·ªã
    num_classes = len(classes)
    fig = plt.figure(figsize=(12, 2*num_classes))
    
    for i, class_name in enumerate(classes):
        images = class_images[class_name]
        
        if not images:
            continue
        
        samples = images[:num_samples] if len(images) > num_samples else images
        
        for j, image_name in enumerate(samples):
            idx = i * num_samples + j + 1
            ax = fig.add_subplot(num_classes, num_samples, idx)
            
            # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n
            if class_name == 'all_images':
                image_path = os.path.join(data_dir, image_name)
            else:
                image_path = os.path.join(data_dir, class_name, image_name)
            
            img = cv2.imread(image_path)
            
            if img is not None:
                if len(img.shape) == 3:
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                
                ax.imshow(img, cmap='gray' if len(img.shape) == 2 else None)
                ax.set_title(f"{class_name}" if class_name != 'all_images' else image_name[:15])
                ax.axis('off')
    
    plt.tight_layout()
    plt.show()
    print(f"‚úì Hi·ªÉn th·ªã {num_classes} class v·ªõi t·ªëi ƒëa {num_samples} m·∫´u/class")

# ==================== T·∫†O FILE C·∫§U H√åNH ====================
def create_dataset_yaml(use_all: bool = False):
    """
    T·∫°o file dataset.yaml cho YOLO Segmentation training
    
    Args:
        use_all: N·∫øu True, d√πng to√†n b·ªô train data cho c·∫£ training v√† validation
    
    Returns:
        dataset_yaml_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file yaml ƒë√£ t·∫°o
    """
    os.makedirs(TRAIN_DIR, exist_ok=True)
    
    dataset_yaml_path = os.path.join(TRAIN_DIR, 'dataset.yaml')
    val_path = 'train/images' if use_all else 'val/images'
    
    dataset_yaml_content = f"""\
# YOLOv8 Segmentation Dataset Configuration
# Sign Language Translation - 22 ASL Letters

path: {DATASET_DIR}
train: train/images
val: {val_path}

# Number of classes
nc: 22

# Class names (ASL letters, excluding F and J)
names:
  - A
  - B
  - C
  - D
  - E
  - G
  - H
  - I
  - K
  - L
  - M
  - N
  - O
  - P
  - Q
  - R
  - S
  - T
  - U
  - V
  - X
  - Y
"""
    
    with open(dataset_yaml_path, "w", encoding='utf-8') as f:
        f.write(dataset_yaml_content)
    
    print(f"‚úì ƒê√£ t·∫°o dataset.yaml t·∫°i: {dataset_yaml_path}")
    print(f"  - Training path: train/images")
    print(f"  - Validation path: {val_path}")
    print(f"  - use_all={use_all}")
    
    return dataset_yaml_path

# ==================== TRAINING ====================
def train_model(epochs=50, batch=16, imgsz=640, model_name='yolov8n-seg.pt', use_all: bool = False):
    """
    Hu·∫•n luy·ªán YOLO Segmentation model
    
    Args:
        epochs: S·ªë epochs training
        batch: Batch size
        imgsz: K√≠ch th∆∞·ªõc ·∫£nh input
        model_name: T√™n pretrained model (m·∫∑c ƒë·ªãnh: yolov8n-seg.pt)
        use_all: D√πng to√†n b·ªô data cho training
    
    Returns:
        model: YOLO model ƒë√£ train
        results: K·∫øt qu·∫£ training
    """
    print("=" * 60)
    print("üöÄ B·∫ÆT ƒê·∫¶U TRAINING YOLOv8 SEGMENTATION MODEL")
    print("=" * 60)
    
    # T·∫°o dataset.yaml
    dataset_yaml_path = create_dataset_yaml(use_all=use_all)
    
    # Load pretrained segmentation model
    model = YOLO(model_name)
    print(f"\n‚úì ƒê√£ load pretrained model: {model_name}")
    print(f"  - Task: Segmentation")
    print(f"  - Architecture: YOLOv8")
    
    # Ki·ªÉm tra dataset.yaml
    if not os.path.exists(dataset_yaml_path):
        raise FileNotFoundError(f"‚ùå dataset.yaml kh√¥ng t√¨m th·∫•y: {dataset_yaml_path}")
    
    # Ki·ªÉm tra dataset structure
    train_images = os.path.join(DATASET_DIR, 'train', 'images')
    train_labels = os.path.join(DATASET_DIR, 'train', 'labels')
    val_images = os.path.join(DATASET_DIR, 'val', 'images')
    
    if not os.path.exists(train_images):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y: {train_images}")
    if not os.path.exists(train_labels):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y: {train_labels}")
    
    # C·∫£nh b√°o n·∫øu kh√¥ng c√≥ validation data
    if not use_all and not os.path.exists(val_images):
        print(f"\n‚ö†Ô∏è  WARNING: Validation path '{val_images}' kh√¥ng t·ªìn t·∫°i!")
        print(f"   Khuy·∫øn ngh·ªã: ƒê·∫∑t use_all=True ho·∫∑c chu·∫©n b·ªã validation data.")
        response = input("   Ti·∫øp t·ª•c training? (y/n): ")
        if response.lower() != 'y':
            print("‚ùå Training b·ªã h·ªßy.")
            return None, None
    
    # ƒê·∫øm s·ªë ·∫£nh v√† labels
    num_train_images = len([f for f in os.listdir(train_images) 
                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
    num_train_labels = len([f for f in os.listdir(train_labels) 
                           if f.endswith('.txt')])
    
    print(f"\nüìä Dataset Statistics:")
    print(f"  - Training images: {num_train_images}")
    print(f"  - Training labels: {num_train_labels}")
    print(f"  - Classes: 22 (ASL letters)")
    
    print(f"\n‚öôÔ∏è  Training Configuration:")
    print(f"  - Epochs: {epochs}")
    print(f"  - Batch size: {batch}")
    print(f"  - Image size: {imgsz}")
    print(f"  - Task: segment")
    
    # Train model
    print("\n" + "=" * 60)
    print("üèãÔ∏è  STARTING TRAINING...")
    print("=" * 60 + "\n")
    
    results = model.train(
        data=dataset_yaml_path,
        epochs=epochs,
        imgsz=imgsz,
        batch=batch,
        project=MODELS_DIR,
        name='sign_language_model',
        task='segment',
        patience=10,  # Early stopping
        save=True,
        plots=True
    )
    
    print("\n" + "=" * 60)
    print("‚úÖ TRAINING HO√ÄN T·∫§T!")
    print("=" * 60)
    print(f"üìÅ Model ƒë∆∞·ª£c l∆∞u t·∫°i: {MODELS_DIR}/sign_language_model/weights/best.pt")
    
    return model, results

# ==================== D·ª∞ ƒêO√ÅN ====================
def predict_image(model, image_path, save_result=True):
    """
    D·ª± ƒëo√°n tr√™n m·ªôt ·∫£nh v·ªõi YOLOv8 Segmentation
    
    Args:
        model: YOLO segmentation model ƒë√£ train
        image_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn ·∫£nh
        save_result: L∆∞u k·∫øt qu·∫£ hay kh√¥ng
    
    Returns:
        image_with_prediction: ·∫¢nh v·ªõi k·∫øt qu·∫£ d·ª± ƒëo√°n
    """
    image = cv2.imread(image_path)
    
    if image is None:
        print(f"‚ùå L·ªói: Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh t·ª´ {image_path}")
        return None
    
    # D·ª± ƒëo√°n
    results = model(image, verbose=False)
    
    # L·∫•y boxes v√† masks
    boxes_arr = results[0].boxes.xyxy.cpu().numpy() if hasattr(results[0].boxes, 'xyxy') else np.array([])
    confs = results[0].boxes.conf.cpu().numpy() if hasattr(results[0].boxes, 'conf') else np.array([])
    cls_ids = results[0].boxes.cls.cpu().numpy().astype(int) if hasattr(results[0].boxes, 'cls') else np.array([])
    
    image_with_prediction = image.copy()
    
    if boxes_arr.size > 0:
        for i, box in enumerate(boxes_arr):
            x1, y1, x2, y2 = box.astype(int)
            confidence = float(confs[i]) if i < len(confs) else 0.0
            class_id = int(cls_ids[i]) if i < len(cls_ids) else -1
            predicted_sign = model.names[class_id] if (hasattr(model, 'names') and class_id in model.names) else f"Class {class_id}"
            
            # V·∫Ω bounding box
            color = (0, 255, 0)
            cv2.rectangle(image_with_prediction, (x1, y1), (x2, y2), color, 2)
            
            # V·∫Ω text
            text = f"{predicted_sign}: {confidence:.2f}"
            cv2.putText(image_with_prediction, text, (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        
        print(f"‚úì Ph√°t hi·ªán {len(boxes_arr)} ƒë·ªëi t∆∞·ª£ng")
    else:
        print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng n√†o")
        cv2.putText(image_with_prediction, "No objects detected", (20, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£ (v·ªõi error handling cho headless env)
    try:
        cv2.imshow("Prediction Result - YOLOv8-seg", image_with_prediction)
        cv2.waitKey(0)
        cv2.destroyAllWindows()
    except Exception as e:
        print(f"‚ÑπÔ∏è  GUI kh√¥ng kh·∫£ d·ª•ng (headless environment): {e}")
        print("   K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o file thay v√¨ hi·ªÉn th·ªã.")
    
    # L∆∞u k·∫øt qu·∫£
    if save_result:
        os.makedirs(MODELS_DIR, exist_ok=True)
        output_path = os.path.join(MODELS_DIR, 'prediction_result.jpg')
        cv2.imwrite(output_path, image_with_prediction)
        print(f"‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ t·∫°i: {output_path}")
    
    return image_with_prediction

# ==================== MAIN ====================
if __name__ == "__main__":
    print("=" * 60)
    print("ü§ñ SIGN LANGUAGE TRANSLATION - YOLOv8 SEGMENTATION")
    print("=" * 60)
    print(f"üìÅ Base Directory: {BASE_DIR}")
    print(f"üìÅ Dataset Directory: {DATASET_DIR}")
    print(f"üìÅ Models Directory: {MODELS_DIR}")
    print(f"üîß Model: YOLOv8 Segmentation (yolov8n-seg.pt)")
    print("=" * 60)
    
    # Menu l·ª±a ch·ªçn
    print("\nCh·ªçn ch·ª©c nƒÉng:")
    print("1. T·∫°o file dataset.yaml")
    print("2. Visualize dataset")
    print("3. Train model (YOLOv8-seg)")
    print("4. Predict tr√™n ·∫£nh")
    print("0. Tho√°t")
    
    choice = input("\nNh·∫≠p l·ª±a ch·ªçn c·ªßa b·∫°n: ").strip()
    
    if choice == "1":
        use_all = input("D√πng to√†n b·ªô data cho training? (y/n, m·∫∑c ƒë·ªãnh n): ").strip().lower() == 'y'
        create_dataset_yaml(use_all=use_all)
    
    elif choice == "2":
        visualize_dataset()
    
    elif choice == "3":
        epochs = int(input("Nh·∫≠p s·ªë epochs (m·∫∑c ƒë·ªãnh 50): ").strip() or 50)
        batch = int(input("Nh·∫≠p batch size (m·∫∑c ƒë·ªãnh 16): ").strip() or 16)
        use_all = input("D√πng to√†n b·ªô data cho training? (y/n, m·∫∑c ƒë·ªãnh n): ").strip().lower() == 'y'
        model, results = train_model(epochs=epochs, batch=batch, use_all=use_all)
    
    elif choice == "4":
        model_path = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n model (ƒë·ªÉ tr·ªëng ƒë·ªÉ d√πng best.pt): ").strip()
        if not model_path:
            model_path = os.path.join(MODELS_DIR, 'sign_language_model', 'weights', 'best.pt')
        
        image_path = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n ·∫£nh: ").strip()
        
        if os.path.exists(model_path) and os.path.exists(image_path):
            model = YOLO(model_path)
            predict_image(model, image_path)
        else:
            if not os.path.exists(model_path):
                print(f"‚ùå Model kh√¥ng t·ªìn t·∫°i: {model_path}")
            if not os.path.exists(image_path):
                print(f"‚ùå ·∫¢nh kh√¥ng t·ªìn t·∫°i: {image_path}")
    
    elif choice == "0":
        print("üëã T·∫°m bi·ªát!")
    
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")